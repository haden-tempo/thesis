%TODO: 'm' refers to the numbebr of batches and 's' refers to the number of options. Must not mix up!
%TODO: Everyone should be 'she'?
\label{bdoodle:chapter}

Scheduling an event for a group of invitees is a frustrating task; it tends to be tedious and time consuming.
A typical scheduling process can be described as iterative approval voting.
First, an event organizer selects a candidate set of date/time options, and asks invitees to respond with their availability. 
Given the responses, the organizer then chooses an agreeable option and announces it, or she may repeat the process by proposing another set of options if no feasible option is found. 
Naturally the organizer and her invitees wish to reach an agreement within a small number of iterations and proposed options -- the more iterations and proposed options there are, the more laborious a scheduling process becomes. 

There exist several software tools that are designed to help an event organizer handle a scheduling process more efficiently -- one of the most well-known tools is Doodle~\footnote{http://www.doodle.com}. 
In Doodle, an organizer can simply list as many date/time options as she likes, 
and each invitee is asked to respond with her availability. 
Essentially, invitees participate in approval voting on all options proposed by the organizer.
Hence if too many options are proposed, then invitees are given the burden of answering them all. 
This often leads to undesired behaviors of invitees such as herding or procrastination, instead of honest, quick responses~\cite{zou2015strategic}. 
On the other hand, if the organizer proposes only few options, there may not exist an agreeable outcome after all, which may result in another iteration of proposals and responses. 
In fact, surveys find that the most challenging part of group scheduling is due to ``chasing people who do not answer'' and ``finding a suitable time.''~\footnote{http://en.blog.doodle.com/2012/07/26/new-findings-a-small-number-of-initiators-organize-most-of-the-meetings/}

Doodle has many practical advantages. 
One of them is its simplicity -- invitees simply need to approve a subset of options based on their availability.
Another is a short duration of scheduling process -- the duration it takes until
the last invitee responds and the meeting time is settled. Each invitee
needs to respond only once, which limits the degree to which the
process is hijacked by invitees' delayed responses. But Doodle has many
drawbacks, even in this idealized setting. In particular, Doodle
forces the invitees to examine many potential date/time options. This can be
inconvenient not so much due to the effort involved (though that is a
factor), but mostly because invitees need to block their available slots
off until a option is announced -- in a way, their `free time' is being hostaged until the final schedule is set.
To quantify this inconvenience, we use the expected number of time
slots floated by the organizer as a proxy.

In order to avoid incurring much inconvenience, 
the organizer can select just a handful number of options,
and poll the invitees about those. If a feasible option is not found
among them, then repeat with another batch of date/time options. 
We call this broad class of polling mechanisms \emph{B-Doodle} (for ``Batched Doodle").
Clearly, Doodle is a special case with one batch consisting of all
options. Another extreme case is the OAAT (one-at-a-time)
mechanism, in which the organizer tests a single option at each
iteration. Doodle minimizes the total number of iterations, whereas 
OAAT minimizes (expected) inconvenience caused by the scheduling process.
In-between lie many other mechanisms, with different
batching schemes, that trade off time against inconvenience
differently.

Figure~\ref{bdoodle:fig:Pareto_scatter} illustrates this via a simple example.
In this scenario there are six options, four invitees, each of whom
is available at each option independently with probability $.8$.
The figure depicts a scatter plot of all 32 different B-Doodle mechanisms,
including Doodle and OAAT. The vertical axis depicts the expected
number of rounds to determine the option, and the horizontal axis
the expected inconvenience.

\begin{figure}[h!] \small
\centering
\includegraphics[scale=0.48]{plots/pareto_scatter_fin.eps}
\caption{A scatter plot of B-Doodle Mechanisms given four invitees and six options.}
\label{bdoodle:fig:Pareto_scatter}
\end{figure}

We can clearly observe a time-inconvenience Pareto frontier in Figure~\ref{bdoodle:fig:Pareto_scatter}. 
In additino, if there is an overall cost function combining time
and inconvenience, one can identify an optimal B-Doodle mechanism
along this frontier as illustrated in
Figure~\ref{bdoodle:fig:Pareto_objective}. 
If we assume that the overall cost is defined as 3$\cdot$\Time+\Inconveniences (which is a linear combination of the two), the optimal mechanism happens to be the ``Half-n-half" mechanism; this mechanism
sends out $3$ options in the first batch, and if no feasible time
slot is found then sends out the remaining $3$ options in the next
batch.

\begin{figure}[h!] \small
\centering
\includegraphics[scale=0.48]{plots/pareto_objective_fin.eps}
\caption{Pareto-frontier and objective function.}
\label{bdoodle:fig:Pareto_objective}
\end{figure}

In this chapter we will investigate the difficulty of finding an optimal B-Doodle mechanism, and to what degree it improves on the simple Doodle in realistic scenarios.
Under the assumption that the set of events in which an invitee is available for a particular option is mutually independent, 
we provide an efficient recursive algorithm for computing an optimal polling mechanism for a broad class of objective functions.

In addition we also  assume that the event organizer is given probability estimates on availability of agents, but it is not clear how one can obtain such probabilities. 
 Probability estimation is an interesting and challenging research question on its own, and we do not attempt to solve the question in this work. However, we provide several plausible methods for estimating the probabilities in the context of group scheduling, which can enable our model and algorithm to be deployed as a real-world application in the future.
 In the psychology literature, Mann et al. found that cultural differences between the Western, individualistic countries (such as the United States) and the Eastern, collectivistic countries (such as China and Japan) lead to different behaviors of respondents when it comes to a group-decision making process~\cite{mann1998cross}.
 More recently, Reinecke et al. analyzed more than 1.5 million Doodle date/time polls from 211 countries, and confirmed similar findings regarding time perception and group's behavior~\cite{reinecke2013doodle}. Among others, they found that ``in comparison to predominantly individualist societies, poll participants from collectivist countries respond earlier, agree to fewer options but find more consensus,'' which agrees with the findings of Mann et al.
 Besides the cultural differences, Doodle's own surveys on event scheduling found that people tend to respond to the scheduling surveys on Mondays, while Monday is the least popular day for having a meeting.~\footnote{http://en.blog.doodle.com/2012/05/23/mondays-for-planning-busy-weekends/}
 We believe that these studies and findings can be used to design a reasonable estimator for availability of agents, by utilizing the features that are known to be crucial -- such as demographics of the group and purpose of the event being scheduled.
 Recent work by Zou et al. analyzed over 340,000 Doodle polls data to study behavioral patterns of the users, and they were able to identify response functions that match the response patterns observed in the real data~\cite{zou2015strategic}. We believe that a similar approach can be taken to tackle the problem of probability estimation in the context of group scheduling.


\section{Notation and Definitions}
We mentioned two dimensions of optimality in the scheduling process: \Times and \Inconvenience.
\Times captures the duration of the scheduling process and \Inconveniences measures how much inconvenience is caused for each invitee during the scheduling process.
In this section we formally define the class of B-Doodle mechanisms and the Batched Doodle Problem (\BDP).

We consider a setting where there are $n$ invitees and $s$ date/time options from which the organizer can choose to propose. The organizer wishes to find a {\em feasible} date/time option that works for at least $\lceil f \cdot n \rceil$ invitees (we call $f\in [0,1]$ the {\em feasibility threshold}) or determines that there is no feasible option.
We model the uncertainty of availability of invitees as a random matrix as follows.
Consider a matrix of mutually independent Bernoulli random variables where rows represent invitees and columns represent a set of date/time options. 
Given probability of success for each Bernoulli random variable, the organizer can ``inspect'' a batch of columns at once to know of the realization of those entries. This corresponds to polling invitees' availability for a batch of date/time options. The organizer wishes to determine with certainty whether the matrix contains a feasible column or it does not. Using the random matrix model, we first investigate interesting properties of an optimal solution, and then discuss how it performs better than the classic Doodle under various settings.
By modeling the setting as a more abstract problem using random matrices, we can apply our model and algorithms to different settings, which we discuss in Section~\ref{bdoodle:sec:discussion}.

 \begin{definition}[Feasibility]
 Let $A$ be an $n$ by $s$ matrix whose entries are from $\{0,1\}$, and refer to the entry of $A$ at row $r$ and column $c$ as $a_{r,c}$.
 Rows represent the invitees, columns represent the options, and an entry being $1$ means the invitee is available for the option. 
 Let $f \in [0,1]$ be the {\em feasibility threshold}.
 We say that a column $c$ of $A$ is {\em $f$-feasible} if at least a fraction $f$ of the entries of the column are 1's.
 We say that $A$ is {\em $f$-feasible} if it contains at least one feasible column.
 We often simply write feasible by dropping $f$.
 \end{definition}
 
In our setting, the organizer iteratively sends out a batch of options until a feasible option is found. 
\Times spent by the scheduling process is measured by the number of iterations and \Inconveniences caused is measured by the number of options that have been sent out by the organizer.

Let us define a class of B-Doodle mechanisms that describe how the organizer sends out options in each iteration.
\begin{definition}[B-Doodle Mechanism]\label{bdoodle:def:BDoodleMechanism}
Let $S = \{1, 2, \dots, s\}$ be a set of $s$ options.
We define a B-Doodle mechanism for $S$ as an ordered partition of $S$.
Let $B = \langle S_1, S_2, \dots, S_m \rangle$ be a partition of $S$ into $m$ subsets such that $S_j \neq\emptyset$ for all $j$, $S_j \cap S_k = \emptyset$ for all $j \neq k$, and $\cup_{l=1}^{m} S_l = S$ where $1 \leq j,k \leq m$.
We define $b_j = |S_j|$ for all $j \leq m$ and call $b_j$ the size of the $j$-th batch.
We write $B_m$ to emphasize that $B$ has $m$ batches.
\end{definition}


We interpret a B-Doodle mechanism $B$ as follows: the organizer proposes the options contained in $S_1$ during the first iteration. If a feasible option is found, the process ends. Otherwise she sends out the next batch, $S_2$, and so on.
Note that there exist exponentially many B-Doodle mechanisms for any a given $S$ (exponential in cardinality of $S$).

The objective in the Batched Doodle Problem (\BDP) is to find an optimal B-Doodle mechanism with minimum expeted cost.
Earlier we considered a simple cost function that is a linear combination of the two.
While this cost function fits in many realistic situations, we want to explore a larger class of cost functions.
We define a class of cost functions that depend only on the number of iterations and batch sizes.
\begin{definition}[Cost function] \label{bdoodle:def:CostFunction}
	A cost function $c$ takes two integers $j$ and $b$ as arguments, and $c(j, b) > 0$ describes the aggregate cost of \Times and \Inconveniences that is incurred by sending out a batch containing $b$ options during the $j$-th iteration. 
	We assume that cost is additive so that the overall cost of executing the first $j$ iterations of ${B}_m = \langle S_1, S_2, \dots, S_m \rangle$ is simply the sum, $\sum_{k=1}^{j} c(k, b_k)$, which we denote by $C(j, {B}_m)$; recall that $b_k = |S_k|$.
A cost function $c$ is said to be $\theta$-\emph{simple} if there exists some constant $\theta>0$ such that $c(j, b) = \theta \cdot c(j-1, b)$ for all $j > 1$ and for all $b \geq 1$.
We often drop $\theta$ and just state that a cost function is \emph{simple}, for brevity.
\end{definition}
It is reasonable to assume that cost is additive (with respect to iterations) because an additional iteration adds the time spent and inconvenience caused for the invitees.
$\theta$-simple cost functions are not as restrictive as they appear.
For instance, when $\theta = 1$, it means that the cost of sending out a batch of a certain size is the same regardless of during which iteration it is being sent. When $\theta > 1$, it means that a later iteration costs more than an earlier iteration if the size of a batch is the same. Lastly, when $\theta \in (0, 1)$, it is the opposite in that an earlier iteration costs more than a later iteration given the same batch size -- this last case may not occur in real life, but our definition naturally allows for such scenarios. 

Note that $C(j, B_{m})$ is strictly increasing in $j$ for any fixed $B_{m}$ because $c(j, b) > 0$ for any $j$ and $b$. While the class of simple cost functions may seem too restrictive, we present a few natural choices of cost functions that belong to the class of simple cost functions. In this work we assume that the underlying cost function is simple; further we assume that $c(j, b)$ for all $1 \leq j, b \leq s$ can be computed in polynomial time with respect to $s$.
\begin{example}[Three simple cost functions] \label{bdoodle:exmp:CostFunctions}
	
	The simplest choice of a cost function is a linear combination of \Times and \Inconvenience, parameterized by some constant $\alpha > 0$; we call this a \emph{linear} cost function. This cost function is $1$-simple. Notice that the first term captures \Times and the second captures \Inconveniences in Equation~\ref{bdoodle:eqn:linear_cost_function}.
	\begin{equation} \label{bdoodle:eqn:linear_cost_function}
		c_{\alpha}(j, b_j) = \alpha + b_j,
		~~	C_{\alpha}(j, {B}_m) = \left(\alpha \cdot j\right) + \left(\sum_{k=1}^{j} b_k\right).
	\end{equation}
	In some cases the organizer may want to penalize the mechanism for executing too many iterations by making later iterations to cost more than earlier iterations. The following describes such cost function, parameterized by some constant $\beta > 1$; we call this a \emph{time-averse} cost function. Notice that the term $\beta^j$ is strictly increasing as $j$ increases (recall $\beta > 1$). This cost function is $\beta$-simple.
	\begin{equation} \label{bdoodle:eqn:time_averse_cost_function}
	c_{\beta}(j, b_j) = \beta^j \cdot b_j,
	~~ C_{\beta}(j, {B}_m) = \left(\sum_{k=1}^{j} \beta^k \cdot b_k \right).
	\end{equation}
	In contrast if the organizer is interested in reducing the size of each batch because it may cause too much inconvenience, then the following cost function could be used, which is parameterized by some constant $\gamma > 1$; we call this an \emph{inconvenience-averse} cost function which is $1$-simple.
	\begin{equation} \label{bdoodle:eqn:inconvenience_averse_cost_function}
		c_{\gamma}(j, b_j) = \gamma^{b_j},
	~~ C_{\gamma}(j, {B}_m) = \left(\sum_{k=1}^{j} \gamma^{b_k} \right).
	\end{equation}
\end{example}

Let us now formally define the Batched Doodle Problem (\BDP).
\begin{definition}[Batched Doodle Problem] \label{bdoodle:def:Problem}
	An instance of the Batched Doodle Problem (\BDP) is a tuple $(A, P_A, f, c)$
	where $A$ is a matrix of Bernoulli random variables,
	$P_A$ is the probability matrix associated with it,
	$f$ is the feasibility threshold with $f \in [0,1]$,
	and $c$ is a cost function.
	An entry $p_{r,c}$ of $P_A$ represents the probability that $a_{r,c}$ is equal to $1$ (we assume $p_{r,c}\in (0,1)$).
	The objective in \BDPs is to find an optimal B-Doodle mechanism $B^*$ such that $B^*$ minimizes the expected cost of the scheduling process (expectation with respect to $P_A$), given $(A, P_A, f, c)$.
\end{definition}

Since there are exponentially many B-Doodle mechanisms, it is impractical to enumerate each B-Doodle and compute its expected cost in order to find an optimal one. In what follows we make some simplifying assumptions that are assumed throughout this paper, and we present an efficient algorithm to solve \BDPs in next section.

% TODO: To change 'invitees' to 'rows' and 'options' to 'columns' and 'availability' to 'entries in A'

\section{Efficient Algorithm for Finding Optimal B-Doodle}
In this section, we discuss a series of useful lemmas, which eventually lead to an efficient algorithm for finding an optimal B-Doodle mechanism in many settings. 

\subsection{Preliminaries}
	Intuitively, if the organizer knows the probability distribution of availability of invitees, then he should inspect the columns that are more likely to be feasible first. We will prove this claim using the following lemma.
	\begin{lemma} \label{bdoodle:lemma:q_t_polytime}
	Given $(A, P_A, f, c)$, let $q_t$ for each column $t$ be the probability that column $t$ is $f$-feasible.
	We can compute $q_t$ in polynomial time.
	\end{lemma}
	\begin{proof}
	For any fixed $t$, let us define $V_t(i, z)$ to be the probability that among the column $t$ of $A$, exactly $z$ entries out of the first $i$ entries (in their row indices) are 1's. $V_t(i, z)$ is well-defined where $1 \leq i \leq n$ and $0 \leq z \leq i$. We further define $V_t(i, z)$ for the following degenerate cases:
	\begin{equation*}
	V_t(i, z) =
	\begin{cases}
		1 & \mbox{if~} i = z = 0 \\
		0 & \mbox{if~} z = -1 \mbox{~or~} (i = 0 \land z > 0)
	\end{cases}
	\end{equation*}
	Then we can compute $V_t(i, z)$ using a simple dynamic programming algorithm according to the following recurrence relation where $1 \leq i \leq n$ and $0 \leq z \leq i$:
	\begin{equation} \label{bdoodle:eqn:vt_precompute}
	V_t(i, z) = V_t(i-1, z-1) \cdot p_{i,t} + V_t(i-1, z) \cdot (1- p_{i,t})
	\end{equation}
	It is easy to verify that the recurrence relation is correct.
	For the degenerate cases (or base cases) when $i = z = 0$, no entries (out of $0$ entries) in column $t$ are equal to 1, and thus $V_t(i, z) = 1$ in this case. When $(z = -1)$ or $(i=0 \land z>0)$, $V_t(i, z) = 0$ because it is impossible to have $z$ entries out of $i$ entries.

	For the main recurrence relation, the entry $a_{r, t}$ is either 1 or 0, and we compute $V_t(i, z)$ by considering both cases. For each fixed $t$, there are $O(n^2)$ entries of $V_t$ to be computed, and computing each entry takes $O(1)$; therefore it takes $O(sn^2)$ time to compute $V_t(i, z)$ for all $t, i, z$.

	Finally we can compute $q_t$ after we compute $V_t(i, z)$ for all $i,z$, as follows:
	
	$$q_t = \sum_{z = \lceil f \cdot n \rceil}^{n} V_t(n, z).$$
	\end{proof}

	Lemma~\ref{bdoodle:lemma:q_t_polytime} allows us to describe a B-Doodle mechanism in a simpler form, along with the following theorem because an optimal B-Doodle mechanism must inspect the columns in non-increasing order of $q_t$.
	\begin{theorem} \label{bdoodle:thm:exchange_argument}
	Consider a B-Doodle mechanism $B$ described by $\langle S_1, S_2, \dots, S_m \rangle$. Suppose that there is some $t \in S_l$ and $t'\in S_{l'}$ with $q_t < q_{t'}$ and $l < l'$. Then $B$ is not an optimal in the sense that we can find another mechanism $B^*$ whose expected cost is strictly less than the expected cost of $B$.
	\end{theorem}
	\begin{proof}
	Given $B$, let us construct $B^* = \langle S^*_1, S^*_2, \dots, S^*_m \rangle$ that is the same as $B$ except that we swap $t$ and $t'$. That is, $S^*_i = S_i$ for all $i \neq l$ and $i\neq l'$, and $S^*_{l} = (S_l \cup t') \setminus t$ and $S^*_{l'} = (S_{l'} \cup t) \setminus t'$. Note that the two mechanisms have the same batch sizes ($b_j = b^*_j$ for all $j$). 

	Let $w_j$ be the probability that the scheduling process ends after the $j$-th iteration if we use $B$, and $w_j^*$ if we use $B^*$. Then $w_j = w_j^*$ for all $j < l$ because $S_j = S^*_j$ for all $j$ with $1 \leq j < l$. Clearly it holds that $w_l < w_l^*$ because of the swap of $t$ and $t'$. For $j$ with $l < j \leq l'$, it holds that $w_j > w_j^*$; this is not obvious, but the intuition is that if $w_l^*$ increases then the subsequent $w_j^*$'s with $l < j \leq l'$ must decrease as they depend on $1-w_l^*$, until this effect is canceled out in the $l'$-th batch. Since the probability of finding no feasible columns during $l'$ iterations is the same in both cases (because $\cup_{i=1}^{l'} S_i = \cup_{i=1}^{l'}S^*_i$) it holds that $w_j = w_j^*$ for all $j > l'$. 

	Let $E_B$ be the expected cost of $B$ and $E_{B^*}$ of $B^*$. Since $b_j = b^*_j$ for all $j$, it holds that $C(j, B) = C(j, B^*)$ for all $j$, and we have:
	\begin{eqnarray}
		E_B - E_{B^*}
		&=& (w_l - w_l^*)C(l, B) + \sum_{j=l+1}^{l'} (w_j - w_j^*)C(j, B) \\
		&=& \sum_{j=l+1}^{l'} (w_j - w_j^*)(C(j, B) - C(l, B))
	\end{eqnarray}
	The first inequality holds by definition of the expected cost and canceling out some terms; the second inequality holds because $(w_l - w_l^*) + \sum_{j=l+1}^{l'} (w_j - w_j^*) = 0$ (as probabilities must add up to one). Since $w_j > w_j^*$ and $C(j, B) > C(l, B)$ for all $j$ (because $C(j, B)$ is an increasing function in $j$ for fixed $B$), we conclude that $E_{B} > E_{B^*}$.
	\end{proof}
	Theorem~\ref{bdoodle:thm:exchange_argument} implies that an optimal B-Doodle that minimizes the expected cost must have the following property: for any two batches $S_j$ and $S_k$ with $j < k$, it must hold that $q_t \geq q_{t'}$ for all $t\in S_j$ and $t' \in S_k$ (otherwise we can apply the theorem and swap the two options to obtain a mechanism with smaller expected cost).
	Therefore we can limit our attention to the sub-class of B-Doodle mechanisms that only describe batch sizes, but not explicitly which options in each batch. Due to Lemma~\ref{bdoodle:lemma:q_t_polytime} we can compute $q_t$ for each $t$ in polynomial time, and thus we can simply sort options by $q_t$ in non-increasing order as a pre-processing step.

	Therefore we can focus on the following sub-class of \emph{simplified} B-Doodle mechanisms when we seek an optimal B-Doodle mechanism.
	\begin{definition}[Simplified B-Doodle Mechanism]
	Let $S = \{1, 2, \dots, s\}$ be a set of $s$ options.
	A simplified B-Doodle mechanism $B$ for $S$ is a vector of integers, described as $B = \langle b_1, b_2, \dots, b_m\rangle$ where $(b_j \geq 1$ for all $j \leq m)$ and $(\sum_{k=1}^{m} b_k = s)$.
	We write $B_m$ to emphasize that $B$ has $m$ batches. It is assumed that $B_m$ partitions $S$ into $m$ batches according to $b_j$'s where options (or columns) are sorted by $q_t$ in non-increasing order.
	\end{definition}
	From now on we use the definition of a simplified B-Doodle, and simply write a B-Doodle mechanism to mean a simplified B-Doodle mechanism but this should cause no confusion.


\subsection{Optimal Algorithm} \label{bdoodle:sec:Algorithm}

	In this section we present an algorithm that finds an optimal B-Doodle mechanism that minimizes the expected cost, given an instance of \BDP.
	Let us first describe how one can express the expected cost of a B-Doodle mechanism.

	Given an instance $(A, P_A, f, c)$, consider some B-Doodle mechanism $B_{m} = \langle b_1, b_2, \dots, b_m \rangle$ with $\sum_{k=1}^{m} b_k = |S|$. Let $\Pr(j)$ be the probability that the scheduling process ends after $j$-th iteration. Let us denote the expected cost of $B_{m}$ by $\Exp_c[B_{m}]$, which can be expressed in terms of $\Pr(\cdot)$ and the cost function $c$:
	\begin{equation} \label{bdoodle:eqn:expCost}
		\Exp_c[B_{m}] = \sum_{j=1}^{m} \Pr(j) \cdot C(j, B_{m})  =  \sum_{j=1}^{m} \Pr(j) \cdot \left(\sum_{k=1}^{j} c(k, b_k)\right)
	\end{equation}%

	While one can compute $\Pr(j)$ in polynomial time, it is not necessary for our algorithm. Instead we present an important lemma that leads us to an efficient algorithm. The following lemma states that if $c$ is simple, then we can compute $\Exp_c[B_{m}]$ in a recursive manner.
	\begin{lemma} \label{bdoodle:lemma:recurrence}
		Consider any mechanism $B_m = \langle b_1, b_2, \dots, b_m \rangle$. Let us denote another mechanism that is obtained after removing the first batch ($b_1$) from $B_m$, as $\hat{B}_{m-1}$ (i.e. $\hat{B}_{m-1} = \langle b_2, b_3, \dots, b_m \rangle$). For each option $t \in S$, let $q_t$ be the probability that $t$ is $f$-feasible.

	If $c$ is a $\theta$-simple cost function with with $\theta > 0$, the following equality holds:
	\begin{equation} \label{bdoodle:eqn:recurrence}
	\Exp_c[B_{m}] = c(1, b_1) +  \left(\prod_{t=1}^{b_1} \left(1 - q_t\right)\right) \cdot \theta \cdot \Exp_c[\hat{B}_{m-1}]
	\end{equation}
	\end{lemma}
	\begin{proof}
	Let us first provide an intuitive way to understand the recurrence relation given by Equation~\ref{bdoodle:eqn:recurrence}. The first term $c(1, b_1)$ captures the cost of sending out the first batch; regardless of when the scheduling process ends, this cost incurs with probability of $1$. If it turns out that the first batch does not contain a feasible option with probability of $(1 - \Pr(1))$, then the organizer must send out the remaining batches, which is precisely $\hat{B}_{m-1}$. It is easy to verify that the product term in the equation is equal to $(1 - \Pr(1))$. The expected cost of $\hat{B}_{m-1}$ is adjusted by a factor of $\theta$ in the equation because the cost function is $\theta$-simple. 

	We now formally prove the claim. Given $B_{m}$ as described above, for each batch $j \in \{1, 2, \dots, m\}$, let $r_j$ be the probability that the $j$-th batch has at least one $f$-feasible option. Let us define $v_0 = 0$ and $v_j = \sum_{k=1}^{j} b_k$ (i.e. $v_j$ is the number of options in batches $1$ through $j$).
	We can express $r_j$ as:
	\begin{equation} \label{bdoodle:eqn:r_j}
	r_j = 1 - \prod_{t=v_{j-1} + 1}^{v_j} (1 - q_t)
	\end{equation}
	$\Pr(j)$ is the probability that the scheduling process ends after $j$-th iteration (with $\Pr(m) = 1 - \sum_{k=1}^{m-1} \Pr(k)$). For $1 \leq j < m$, $\Pr(j)$ is given by:
	\begin{equation} \label{bdoodle:eqn:Pr_j}
	\Pr(j) =  r_j  \prod_{k=1}^{j-1} (1 - r_k)
	\end{equation}
	It is understood that $\prod_{k=x}^{y} (\cdot)$ is equal to $1$ when $x > y$. 

	Recall that $\hat{B}_{m-1} = \langle b_2, b_3, \dots, b_{m} \rangle$.
	Not to be confused, let us express it as $\hat{B}_{m-1} = \langle \hat{b}_1, \hat{b}_2, \dots, \hat{b}_{m-1}\rangle$ with $\hat{b}_j = b_{j+1}$ for all $j\geq 1$. Let $\hat{r}_j$ be the probability that a feasible option exists in $j$-th batch of $\hat{B}_{m-1}$, which is equal to $r_{j+1}$ for all $j \geq 1$. Let $\hat{Pr}(j)$ be the probability that scheduling ends after the $j$-th iteration of $\hat{B}_{m-1}$:
	\begin{equation}
		\hat{Pr}(j)
		= \hat{r}_j \prod_{k=1}^{j-1} (1 - \hat{r}_k)
		= r_{j+1} \prod_{k=2}^{j} (1 - r_{k})  \label{bdoodle:eqn:pr_hat_b}
	\end{equation}
	We can express $\Exp_c[\hat{B}_{m-1}]$ as follows:
	\begin{eqnarray*}
	\Exp_c[\hat{B}_{m-1}]
	&=& \sum_{j=1}^{m-1} \hat{Pr}(j) \left( \sum_{k=1}^{j} c(k, \hat{b}_k) \right) \\
	&=& \sum_{j=1}^{m-1} r_{j+1} \left(\prod_{k=2}^{j} (1 - r_{k})\right) \left(\sum_{k=2}^{j+1} c(k-1, b_{k+1})\right) \\
	&=& \sum_{j=2}^{m} r_{j} \left(\prod_{k=2}^{j-1} (1 - r_{k})\right) \left(\sum_{k=2}^{j} c(k-1, b_{k+1})\right)
	\end{eqnarray*}
	The first equality holds by definition. The second equality is due to Equation~\ref{bdoodle:eqn:pr_hat_b} and because $\hat{b}_k = b_{k+1}$. The third equality is by changing the range of $j$ in the summation. 

	If we multiply $\Exp_c[\hat{B}_{m-1}]$ by $(1 - r_1)\theta$, we get the following:
	\begin{equation} \label{bdoodle:eqn:ExpCost_Bhat}
	(1-r_1)\theta \Exp_c[\hat{B}_{m-1}] = \sum_{j=2}^{m} r_{j} \left(\prod_{k=1}^{j-1} (1 - r_{k})\right) \left(\sum_{k=2}^{j} c(k, b_{k+1})\right)
	\end{equation}
	Notice that the product term now runs from $k = 1$ to $j-1$ as we multiply by $(1-r_1)$ and the inner-most summation has $c(k, b_{k+1})$ as we multiply by $\theta$ (recall that $c$ is $\theta$-simple).

	Finally we can express $\Exp_c[B_m]$ in terms of $\Exp_c[\hat{B}_{m-1}]$ as follows (where $c_1 = c(1, b_1)$ for brevity):
	\begin{eqnarray*}
	\Exp_c[B_{m}]
	&=& \sum_{j=1}^{m} \Pr(j) \left(\sum_{k=1}^{j} c(k, b_k)\right) \\
	&=& c_1 + \left(\sum_{j=2}^{m} r_j \left(\prod_{k=1}^{j-1} (1 - r_k)\right) \left(\sum_{k=2}^{j} c(k, b_k)\right) \right) \\
	&=& c_1 + (1-r_1)\theta \Exp_c[\hat{B}_{m-1}] \\
	&=& c(1, b_1) + \left(\prod_{t=1}^{b_1} (1-q_t) \right)\cdot \theta \cdot \Exp_c[\hat{B}_{m-1}]
	\end{eqnarray*}
	The first equality holds by definition of $\Exp_c[B_{m}]$. The second equality is obtained by taking $c(1, b_1)$ out from the summation (and note that $\Pr(j)$ adds up to 1) first and then applying Equation~\ref{bdoodle:eqn:Pr_j}. The last two inequalities hold due to Equation~\ref{bdoodle:eqn:ExpCost_Bhat} and \ref{bdoodle:eqn:r_j}, respectively.
	The last expression exactly  matches Equation~\ref{bdoodle:eqn:recurrence} in the lemma.
	\end{proof}

	Using the recurrence relation in Lemma~\ref{bdoodle:lemma:recurrence} and the computing method in Lemma~\ref{bdoodle:lemma:q_t_polytime}, we can now design an efficient recursive algorithm finds the optimal B-Doodle mechanism. Our algorithm is presented in Algorithm~\ref{bdoodle:alg:recursive} as a recursive method $Rec(x)$. We assume that the values of $q_t$ have been computed as a pre-processing step prior to running our algorithm, using the method in Lemma~\ref{bdoodle:lemma:q_t_polytime}. We further assume that options are sorted in decreasing order of $q_t$ (i.e. $q_1 \geq q_2 \geq \dots \geq q_{s}$); therefore we simply refer to the option by its index (i.e. $t = 3$ refers to the third option in the sorted list of options).

	Given $1 \leq x \leq s$, $Rec(x)$ returns optimal B-Doodle ($B^*$) that consists of options $\{x, x+1, \dots, s\}$ and its expected cost ($EC^*$). To solve for a given instance of the problem, we simply call the method $Rec(x)$ with $x = 1$.

	\begin{algorithm}
	\caption{Recursive Algorithm: $Rec(x)$}
	\label{bdoodle:alg:recursive}
	\begin{algorithmic}[1]
		\State ${B}^* \gets \langle s-x+1 \rangle$, $EC^* \gets c(1, s-x+1)$
		\For{$b := 1, 2, \dots, s-x$}
			\State $({B}, EC) \gets Rec( x+b )$
			\State $EC_b \gets c(1, b) + \left(\prod_{t=x}^{x+b-1} (1 - q_t)\right)\cdot \theta \cdot EC$
			\If {$EC^* > EC_b$}
				\State $EC^* \gets EC_b$
				\State $B^* \gets \langle b, {B} \rangle$
			\EndIf
		\EndFor
		\State \textbf{return} $({B}^*, EC^*)$
	\end{algorithmic}
	\end{algorithm}

	\begin{theorem} \label{bdoodle:thm:recursive_algo}
	Algorithm~\ref{bdoodle:alg:recursive} runs in polynomial time, and returns an optimal B-Doodle mechanism that minimizes the expected cost, given an instance $(N, S, f, c, P)$ of \BDPs when $c$ is $\theta$-simple.
	\end{theorem}
	\begin{proof}
	Our recursive method is very simple: given options $\{x, x+1, \dots, s\}$, it iteratively considers the case of sending out $b$ options in the first iteration, and computes the expected cost $EC_b$ of doing so, for each $b$ with $1 \leq b \leq s-x+1$.  In line 1, our method checks for the trivial case when $b = s-x+1$ (i.e. all options are sent in a single batch); we store this mechanism in $B^*$ and its expected cost in $EC^*$. Then we iterate $b$ from $1$ to $s-x$ and compute the expected cost $EC_b$, and compare with the optimal expected cost found so far (lines 2-9). For each $b$, we first recursively compute the expected cost for the remaining options (namely, $\{x+b, x+b+1, \dots, s\}$) by calling our method $Rec(x+b)$, and store the expected cost in $EC$ (line 3). Using Lemma~\ref{bdoodle:lemma:recurrence} we can compute $EC_b$ as in line 4 (note that the first batch contains $b$ options from $x$ to $x+b-1$). In lines 5-8, we simply compare $EC_b$ with the current optimal, $EC^*$, and update if necessary; $\langle b, B \rangle$ should be interpreted as the concatenation of $b$ and $B$ into a single vector of integers. Finally in line 10, our method returns the optimal B-Doodle found, $B^*$ and its expected cost $EC^*$. 

	We prove correctness of the algorithm by induction on $x$ (from $x = s$ to $x = 1$). The base case is trivial: if $x = s$ the only B-Doodle mechanism is $\langle 1 \rangle$ and our method finds it in line $1$ and returns it in line $10$. Suppose our method correctly returns the optimal B-Doodle mechanism (and its expected cost) for all $x > k$ for some $k$ (the inductive hypothesis), and we prove for the case of $x = k$. When $x = k$, there are precisely $(s-k+1)$ options that are to be sent, and the first batch can contain any number of options between $1$ and $(s-k+1)$, inclusive. In lines 1-9 our method considers all such cases, and for each case it computes the expected cost correctly (in line 4) due to our inductive hypothesis and Lemma~\ref{bdoodle:lemma:recurrence}. Therefore our method finds the optimal B-Doodle for all $x$ with $1 \leq x \leq s$; in particular when $x = 1$, it returns the desired optimal B-Doodle mechanism for the given problem instance.

	Our recursive method runs in polynomial time when we use memoization on $Rec(x)$ with respect to $x$; that is, for each $x$ we cache what $Rec(x)$ returns for the first time, and for any subsequence calls to $Rec(x)$ we simply return the cached values. Therefore lines 1-9 are executed at most once for each $x$ with $1 \leq x \leq s$. Also note that $Rec(x)$ makes a call to $Rec(x + b)$ with $b \geq 1$ and $x+b \leq s$, which means there are no infinite loops. Once $Rec(x)$ is computed for all $x > k$, it takes $O(|S|^2)$ time to compute $Rec(k)$, and thus the overall running time of our algorithm is $O(|S|^3)$ when we start with $Rec(1)$. Pre-processing steps (of computing $q_t$) also run in polynomial time due to Lemma~\ref{bdoodle:lemma:q_t_polytime}.
	\end{proof}





\section{Experimental Results with Synthetic Data}

We showed that we can find an optimal B-Doodle mechanism that minimizes the expected cost. But a very important question remains: Is our optimal B-Doodle substantially better than Doodle in realistic settings? If the answer is no, we do not have much incentive to discard the simplest mechanism, Doodle, over using our sophisticated algorithm. Intuitively we expect Doodle to be inefficient if there are many options (i.e., $s$ is large) because it causes much inconvenience for invitees to examine the options. Reasoning further, we can also see that inefficiency of Doodle depends on the probability of each option being feasible (i.e. $q_t$ values), which in turn depends on $p_{i, t}$ values. If invitees are relatively free, then a small number of options would be sufficient for finding a feasible option, which makes Doodle inefficient. Last but not least, the underlying cost function plays an important role as well. These all sound plausible, and we validate our intuition with simulation results.

In our setting there are many experimental choices one can choose from. In what follows we look at a representative example in which each invitee is available for each option with the same probability of $p$; that is, $p_{i, t} = p$ for some constant $p$. (While in realistic situations the $p_{i, t}$'s may all differ, we  note that in our simulations  our results seem to carry over to these more general settings as well.) We present experimental results for each of the three cost functions discussed in Example~\ref{bdoodle:exmp:CostFunctions}: The linear cost function, the time-averse cost function, and the inconvenience-averse function.

% ===================== Linear Cost Function (Below)

\subsection{Case of Linear Cost Function} \label{bdoodle:sec:result_linear_cost}
Recall that the linear cost function is parameterized by some constant $\alpha>0$ and that $c_{\alpha}(j, b) = \alpha + b$. We used $\alpha = 2$ as an experimental choice, and we later discuss what we observed for different values of $\alpha$.

We reasoned that Doodle becomes suboptimal if the number of options, $s$, is large. Therefore it is interesting to know for what ranges of $s$, Doodle is suboptimal. For some fixed $(n, s, p)$, let $C^D(n, s, p)$ be the cost of Doodle and $C^*(n, s, p)$ be the expected cost of $B^*$. We want to find the smallest critical point $S^*(n, p)$ such that for any $s \geq S^*(n, p)$ we have $C^D(n, s, p) > C^*(n, s, p)$.

In Table~\ref{bdoodle:table:DoodleSuboptimal}, we show $S^*$ for various $(n, p)$ when $f = 1$ (i.e. the organizer requires everyone be available). For instance we find $S^*(6, .8) = 5$, which implies that Doodle is suboptimal for all $s \geq 5$ given that $n=6$ and $p=.8$. The smaller $S^*$ is, the less practical Doodle is for the corresponding $(n, p)$.
Across the table we can observe that $S^*$ is small when $n$ is small and/or when $p$ is high -- this agrees with our intuition.
We highlighted $8$ entries in boldface to emphasize that $S^* \leq 15$; for the corresponding $(n, p)$ values, Doodle is suboptimal if there are $15$ or more options being considered.
\begin{table}[h!]  % \small
\centering
\begin{tabular}{|c|c|c|c|c|c|}
	\hline
	$S^*$ & $n = 2$ & $n = 4$ & $n = 6$ & $ n = 10 $ & $n = 15$ \\ \hline
	$p = .8$ & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{8} & \textbf{15} \\ \hline
	$p = .5$ & \textbf{5} & \textbf{11} & 22 & 90 & $>300$\\ \hline
	$p = .2$ & \textbf{14} & 70 & $>300$ & $>300$ & $>300$ \\ \hline
\end{tabular}
\caption{Critical point $S^*$ for various $(n, p)$ when $f = 1, \alpha = 2$.
Entries in boldface emphasize that $S^* \leq 15$.
} \label{bdoodle:table:DoodleSuboptimal}
\end{table}

Not only do we want to know when Doodle is suboptimal, but we also want to know how inefficient Doodle is in realistic situations. For some fixed $(n, s, p)$, we define the efficiency of Doodle, $e_{D}(n,s,p)$, as the ratio of the optimal expected cost to the cost of Doodle: $e_{D}(n,s,p) = C^*(n, s, p) / C^D(n, s, p)$.

In Table~\ref{bdoodle:table:DoodleEfficiency} we show $e_{D}$ for the same settings of $(n, p)$ we used in Table~\ref{bdoodle:table:DoodleSuboptimal}. For this experiment we used $s = 15$ and $f = 1$. If $e_{D} = 1$ Doodle is optimal, and if $e_{D}$ is close to zero then Doodle is very inefficient. We find that $e_{D}(2, .8) = .270$, which implies that Doodle is very inefficient in this case; on the other hand $e_{D}(10, .5) = 1$, in which case Doodle is optimal. Across the table we observe the same pattern we observed before -- for small $n$ and high $p$, Doodle is substantially inefficient.
\begin{table}[h]  % \small
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
	\hline
	$e_{D}$ & $n = 2$ & $n = 4$ & $n = 6$ & $ n = 10 $ & $n = 15$ \\ \hline
	$p = .8$ & \textbf{.270} & \textbf{.361} & \textbf{.486} & .777 & .986 \\ \hline
	$p = .5$ & \textbf{.502} & .904 & 1 & 1 & 1  \\ \hline
	$p = .2$ & .970 & 1 & 1 & 1 & 1\\ \hline
\end{tabular}
\caption{Efficiency of Doodle $e_{D}$ for various $(n, p)$ when $f = 1, \alpha=2, s = 15$.
Entries in boldface emphasize that $e_{D} < .750$.
} \label{bdoodle:table:DoodleEfficiency}
\end{table}

Inefficiency of Doodle is more pronounced when the organizer has a lower feasibility threshold such as $f = .7$; in such case, only a fraction of invitees need to be available.
We can clearly observe the worsened inefficiency of Doodle in Table~\ref{bdoodle:table:DoodleEfficiency-lower-attendance}.
Here we show $e_{D}$ values for the same set of $(n, p)$ as in Table~\ref{bdoodle:table:DoodleEfficiency} but with $f = .7$.
Previously we highlighted four entries with $e_{D} < .750$ when $f = 1$ in Table~~\ref{bdoodle:table:DoodleEfficiency}; when $f = .7$ the number of entires with $e_{D} < .750$ doubled to eight.
Note that the first column is identical between the two tables; this is because $f = .7$ still requires both invitees to be available when $n = 2$.
Also notice that $e_{D}$ is surprisingly low in the first row of Table~\ref{bdoodle:table:DoodleEfficiency-lower-attendance} across all columns. This shows that regardless of the number of invitees, Doodle is significantly inefficient when the invitees are highly available ($p \geq .8$) and the organizer has a relaxed feasibility threshold.
\begin{table}[h]  % \small
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
	\hline
	$e_{D}$ & $n = 2$ & $n = 4$ & $n = 6$ & $ n = 10 $ & $n = 15$ \\ \hline
	$p = .8$ & \textbf{.270} & \textbf{.215} & \textbf{.267} & \textbf{.201} & \textbf{.211} \\ \hline
	$p = .5$ & \textbf{.502} & \textbf{.434} & .772 & \textbf{.628} & .913  \\ \hline
	$p = .2$ & .970 & 1 & 1 & 1 & 1\\ \hline
\end{tabular}
\caption{Efficiency of Doodle $e_{D}$ for various $(n, p)$ when $f = .7, \alpha=2, s = 15$.
Entries in boldface emphasize that $e_{D} < .750$.
} \label{bdoodle:table:DoodleEfficiency-lower-attendance}
\end{table}

We ran experiments with different values of $(n, s, p, f, \alpha)$, and observed the same  trends that are presented here.


% ===================== Other Cost Functions (Below)

%\balancecolumns
\subsection{Case of Other Cost Functions}
We present some of the experimental results we obtained with different cost functions, namely the time-averse cost function and the inconvenience-averse cost function. As the names suggest, the former places more weight on optimizing \Time, while the latter on optimizing \Inconvenience. 

For the time-averse cost function (recall $c_{\beta}(j, b) = \beta^j \cdot b$), we chose $\beta = 2$ as an experimental choice. Notice that the cost increases exponentially for later iterations, which forces the organizer to send out a small number of batches (as in Doodle). We ran the same set of experiments as before to measure efficient of Doodle, $e_{D}$.

The result is summarized in Table~\ref{bdoodle:table:DoodleEfficiency-lower-attendance_time_averse}, which shows the same trends as we observed in previous experiments. Notice that in the first row ($p = .8$), as $n$ increases we expect $e_{D}$ to decrease, but $e_{D}$ fluctuates while decreasing in general. The fluctuation is due to the rounding of attendance requirement ($\lceil a \cdot n \rceil$). For instance when $n = 4$, $\lceil a \cdot n \rceil$ = 3 which effectively requires 75\% of attendees be available.
\begin{table}[h]  % \small
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
	\hline
	$e_{D}$ & $n = 2$ & $n = 4$ & $n = 6$ & $ n = 10 $ & $n = 15$ \\ \hline
	$p = .8$ & \textbf{.180} & \textbf{.104} & \textbf{.175} & \textbf{.088} & \textbf{.099} \\ \hline
	$p = .5$ & \textbf{.561} & \textbf{.457} & .876 & \textbf{.726} & .987  \\ \hline
	$p = .2$ & 1 & 1 & 1 & 1 & 1\\ \hline
\end{tabular}
\caption{Efficiency of Doodle $e_{D}$ for various $(n, p)$ when $f = .7, \beta=2, s = 15$.
Entries in boldface emphasize that $e_{D} < .750$.
} \label{bdoodle:table:DoodleEfficiency-lower-attendance_time_averse}
\end{table}

For the inconvenience-averse cost function (recall $c_{\gamma}(j, b) = \gamma^{b}$), we chose $\gamma = 1.1$ as an experimental choice. While $\gamma$ is fairly small, because the cost function is an exponential function of $b$, an optimal B-Doodle must send out small-size batches. This cost function optimizes \Inconveniences primarily. We ran the same set of experiments as before to measure efficiency of Doodle, $e_{D}$.

The result is summarized in Table~\ref{bdoodle:table:DoodleEfficiency-lower-attendance_inconvenience_averse}. Notice that $e_{D}$ is not equal to $1$ even when $p = .2$ in this setting, which we did not observe with the other cost functions previously. Due to integer-rounding, we again observe some fluctuations in $e_{D}$ across columns within the same row, but the general trend is that $e_{D}$ decreases as $n$ increases.
\begin{table}[h]  % \small
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
	\hline
	$e_{D}$ & $n = 2$ & $n = 4$ & $n = 6$ & $ n = 10 $ & $n = 15$ \\ \hline
	$p = .8$ & \textbf{.333} & \textbf{.299} & \textbf{.329} & \textbf{.294} & \textbf{.298} \\ \hline
	$p = .5$ & \textbf{.499} & \textbf{.450} & \textbf{.695} & \textbf{.592} & .799  \\ \hline
	$p = .2$ & .850 & .887 & .974 & .976 & .980 \\ \hline
\end{tabular}
\caption{Efficiency of Doodle $e_{D}$ for various $(n, p)$ when $f = .7, \gamma=1.1, s = 15$.
Entries in boldface emphasize that $e_{D} < .750$.
} \label{bdoodle:table:DoodleEfficiency-lower-attendance_inconvenience_averse}
\end{table}

%\balancecolumns

\subsection{Summary of Experiments}
While we only presented experimental results with specific values of $(n, s, p, f)$ and parameters $(\alpha, \beta, \gamma)$, we observed that Doodle is in general substantially inefficient, including (but not limited to) when one or more of the following conditions hold:
\begin{itemize}
	\item There is a relatively small number of invitees ($n \leq 10$).
	\item There is a large number of options ($s \geq 15$).
	\item invitees are relatively free ($p > .5$).
	\item Feasibility threshold is relaxed ($f < .8$).
	\item The cost function places more weight on \Inconveniences than \Times ($\alpha < 20$, $\beta < 5$, or $\gamma > 1.05$).
\end{itemize}
Intuitively the first four conditions affect $q_t$ (the probability that option $t$ is feasible) in the same way, and if $q_t$'s are high then Doodle is more likely to be inefficient because a few options may be sufficient for finding a feasible one. While the last condition is independent of $q_t$, the cost function determines what is being optimized, and Doodle becomes more inefficient if $c$ favors reducing \Inconveniences over reducing \Time.


\section{Discussion}
\label{bdoodle:sec:discussion}

In this chapter we identified two important dimensions of optimality in
group scheduling: \Times and \Inconvenience. We generalized the popular
Doodle mechanism to a class of B-Doodle mechanisms that partition date/time
options into batches. We showed an example of the Pareto-frontier of
B-Doodle mechanisms on the \Time-\Inconveniences dimensions. We then
described an efficient algorithm for finding an optimal B-Doodle
mechanisms, given a simple cost function that aggregates \Times and
\Inconvenience, assuming probabilistic independence among availability
of invitees. We showed in simulations that optimal B-Doodle mechanism
is superior to Doodle in realistic situations, sometimes greatly so.


% TODO: Move to next chapter?
% While useful in and of itself, this work leaves open many questions.
% Here we focused on a sub-class of the Single-proposer mechanisms in
% which there is a sole organizer who is trying to find an agreeable
% schedule. In realistic situations a group of invitees often negotiate
% among themselves or respond with counter-offers. It will be
% interesting to extend our B-Doodle mechanisms to the Multi-proposer
% mechanisms. Another obvious extension of our work is to relax the
% independence assumption and develop an adaptive mechanism that infers
% availability of invitees based on their responses. In addition, we
% implicitly assumed that invitees prefer all options equally (by
% allowing invitees to specify their availability but not preferences),
% but people often have specific preferences. Thus it will be important
% to study the trade-off between optimizing the cost of scheduling
% process and finding a `good' schedule.
% Finally, we assumed honesty, promptness, and trust of invitees, but
% these are strong assumptions. In particular, promptness assumes that
% invitees do not procrastinate and do not deliberately delay responses.
% It is arguably true that if the scheduling process is efficient then
% it will indeed reduce the procrastination of invitees. However in
% practice an invitee may have an incentive to delay her response or to
% lie about her availability or preferences. Practical mechanisms should
% not be vulnerable to such strategic behavior of invitees, and this is
% yet another direction for future work.
%
