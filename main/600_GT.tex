\label{GT:chapter}


We have so far assumed that the event organizer knows of or can query preferences and constraints of agents, but in some settings this may not be a realistic assumption. For instance, if agents are strategic, the organizer can no longer assume that agents will report their preferences and constraints truthfull because lying may lead to a better outcome than telling the truth would.

To understand why strategic behavior of agents can be troublesome, let us consider a small example that features two agents in the Stable Invitations Problem.

\begin{example} \label{GT:eg:strategicAgents}
		Consider two agents with identical preferences on the number of participants with no friends or enemies. 
	\begin{equation*}
		\begin{aligned}
				S_1 = \{1\}, ~~& S_2 = \{1\}
		\end{aligned}
	\end{equation*}

Note that both agents have decreasing preferences in this example. 
There are two stable invitations, namely $I_1 \{a_1\}$ and $I_2 = \{a_2\}$. 
The empty invitation ($\emptyset$) is not stable while the full invitation ($\{a_1,a_2\}$) is not individually rational. 

If the organizer were to choose $I_1$ given preferences of agents, then $a_2$ would have an incentive to lie -- if $a_2$ had reported her preference as $\hat{S}_2 \{1, 2\}$ instead of $S_2$, then $I_2$ would have been the only stable invitation given $(S_1, \hat{S}_2)$. Clearly, $a_2$ prefers $I_2$ over $I_1$, and therefore $a_2$'s best action in hindsight would have been lying about her preference. By symmetric, the same holds for $a_1$, if the organizer were to choose $I_2$ given $(S_1, S_2)$.
\end{example}

In this chapter we investigate how strategic behavior of agents can complicate the problem of finding a stable solution in the Stable Invitations Problem and the Group Activity Selection Problem. We begin by considering the least complex problem, which is the intersection of these two problems -- agents have anonymous preferences (i.e., no friends and enemies) and there is only one activity. This is a fairly restrictive assumption, but it reveals interesting interactions between strategic behavior of agents and the objectives in these problems. In fact, as we shall see, it is in general impossible to design a mechanism under which truth-telling is a dominant strategy. In a more restrictive setting, however, where all agents have increasing preferences (i.e., all agents prefer more participants than fewer), we can design a mechanism under which truth-telling is a dominant strategy in addition to many other desirable properties. 



\section{Definitions and Notation} \label{GT:sec:prelim}

Recall from Definition~\ref{SIP:def:instance} that an instance of the Stable Invitations Problem is given by a set of agents $N = \{a_1, a_2, \dots, a_n\}$, a set of friends $F_i$, a set of enemies $R_i$, and an approval set $S_i$ for each agent $a_i$.

We first focus on the special case of this problem when all friend sets and enemy sets are empty sets -- we call this the Anonymous Stable Invitations Problem (\ASIP), which is known to be solvable in polynomial time; this problem is also equivalent to the special case of the Group Activity Selection Problem with just one activity. We then discuss how our technical results extend to the Stable Invitations Problem and the Group Activity Selection Problem.

\begin{definition}
	The Anonymous Stable Invitations Problem (\ASIP) is a special case of the Stable Invitations Problem (and of the Group Activity Selection Problem), in which no agent has friends or enemies. An instance of \ASIPs is given by a set of agents $N = \{a_1, a_2, \dots, a_n\}$ and an approval set $S_i$ for each agent.
\end{definition}

In this chapter we focus on finding a stable invitation whose definition was given in Section~\ref{SIP:sec:SIP:prelim} by assuming that $F_i = R_i = \emptyset$ for every agent $a_i\in N$. 

Let us formally define a deterministic mechanism in the context of \ASIPs with strategic agents, and define what constitutes a strategy-proof mechanism. We later discuss in Sectino~\ref{GT:sec:randomized} why our technical results do not change when we consider randomized mechanisms.

\begin{definition} \label{GT:def:mechanism}
Given an instance of \ASIP, we define $V_i$ (the set of available actions to $a_i$) to be the set of all subsets of the set of outcomes, $[1, n]$.\footnote{In the Anonymous Stable Invitations Problem, agents only care about the size of an invitation, not participants.} 
A (deterministic) \emph{mechanism} $\mathcal{M}$ is a pair $(V, Z)$ where $V = (V_1 \times \cdots \times V_n)$ is the set of action profiles of all agents (i.e., $V_i \subset [1,n]$) and $Z: V \mapsto U$ is a mapping from each action profile to an invitation in $U$ where $U = 2^{N}$. 
Let $V_{-i} = (V_1 \times \cdots \times V_{i-1} \times V_{i+1} \times \cdots \times V_{n})$ be the set of action profiles available to all agents but agent $a_i$. A mechanism $\mathcal{M} = (V, Z)$ is said to be \emph{strategy-proof} if for all $a_i\in N$, $a_i$ prefers the invitation $Z(S_i, v_{-i})$ over the invitation $Z(v_i, v_{-i})$ for all $v_i \in V_i$ and $v_{-i} \in V_{-i}$.
\end{definition}

In words, mechanism $\mathcal{M}$ is strtaegy-proof if for every agent $a_i\in N$, truth-telling is a dominant strategy for any against any strategy profile of other agents given any type of agent $a_i$ (i.e., any preference of $a_i$). Due to the revelation principle, we only need to consider direct mechanims in which agents simply report their types (or preferences, in our setting). For more information, please see BLAH. (TODO).


\section{Main Technical Results} \label{GT:sec:Mechanism}

Let us present our first impossibility result which states that strategy-proofness and stability of an invitation are not compatible in \ASIP. 

%%%% ========== Impossibility Results
\begin{theorem} \label{GT:thm:impossibility}
No strategy-proof mechanism can find a stable invitation (even if it exists) given arbitrary instances of \ASIPs when the number of agents is at least two.
\end{theorem}
\begin{proof}
Example~\ref{GT:eg:strategicAgents} can serve as a proof; no strategy-proof mechanism can find a stable invitation for this instance.
\end{proof}

Intuitively, this can be explained by conflicting interests of the organizer and agents -- the organizer is trying to maximize attendance while the agents (with \DEC-preferences) to minimize. 
This intuition begs the question: What if all agents have \INC-preferences instead?
When all agents have \INC-preferences, we obtain a positive result in this case as Theorem~\ref{GT:thm:mechanism} states. 

% Since no strategy-proof mechanisms can find a stable invitation,
% one can instead seek to design a strategy-proof mechanism that can find a non-empty IR invitation.
% We show that this is also impossible as the following theorem states.
%
% \begin{theorem} \label{GT:thm:impossibility_IR}
% 	No strategy-proof mechanism can find a non-empty individually rational (IR) invitation, even if it exists, for arbitrary instances of \ASIP.
% \end{theorem}
% \begin{proof}%TODO: This example requires that agents have ordinal preferences (agent 2 and 3). Can I find another example?
% Consider three agents with their approval sets shown as follows:
% \begin{equation*}
% 		S_1 = \{3\},~~ S_2 = \{2, 3\}, ~~ S_3 = \{2, 3\}.
% \end{equation*}
% There are two non-empty IR invitations: $I_1 = \{a_1, a_2, a_3\}$ and $I_2 = \{a_2, a_3\}$.
% If a mechanism chooses $I_1$ given $(S_1, S_2, S_3)$, then $a_2$ can report $\hat{S}_2 = \{2\}$ instead, which makes $I_2$ the only non-empty IR invitation given $(S_1, \hat{S}_2, S_3)$.
%  Similarly, if a mechanism chooses $I_2$ given $(S_1, S_2, S_3)$, then $a_3$ can report $\hat{S}_3=\{3\}$ instead to make $I_3$ the only non-empty IR invitation. The rest of the proof is similar to that of Theorem~\ref{GT:thm:impossibility}.
% \end{proof}
% Earlier we emphasized that the conflict between agent(s) and the organizer is the main factor that leads to an impossibility result.
% Indeed, in the example we used in the proof of Theorem~\ref{GT:thm:impossibility}, both agents have \DEC-preferences while the organizer's goal is to maximize attendance.


\begin{theorem} \label{GT:thm:mechanism}
	There is a strategy-proof mechanism for \INC-instances of \ASIP, which can also find a stable invitation in linear time (after sorting) and its size is maximum among all stable invitations.
\end{theorem}
\begin{proof}[Proof sketch] %TODO: To do formal proof. Also include anecdnote from Matt Jackson.
For simplicity let us assume that each agent reports her threshold value (i.e., $l_i$ as defined in Definition (to be added)) as $L_i$ ($L_i$ may differ from $l_i$). 
Our mechanism then chooses the largest $k$ such that $L_k \leq k$ holds and chooses the set of $k$ agents with largest threshold values (if no such $k$ exists, mechanism chooses the empty invitation). 
\end{proof}

%TODO: The following paragraph is not necessary, once formal proof is added.
	Although our mechanism is simple, proof of Theorem~\ref{GT:thm:mechanism} is not trivial. First, even though all agents have \INC-preferences, a full invitation is not necessarily stable (if at least one agent is unwilling to attend at all). Second, an agent may have an incentive to under-report that results in having the organizer to invite more agents than when the agent had truthfully reported. It is indeed possible for an agent to under-report ($L_i < l_i$) and lead to a larger invitation, but we show that the size of the larger invitation would still be less than $l_i$ (see Appendix for a proof). 


\subsection{Randomized Mechanisms} \label{GT:sec:randomized}
Although we have so far only discussed deterministic mechanisms, our impossibility results can be extended to randomized mechanisms. First we define $Z$ to be a mapping from $V$ to $\Pi(U)$ where $\Pi(U)$ denotes the set of all probability distributions over $U$. The definition of a strategy-proof mechanism must change accordingly -- we do this by adopting the axioms in the von Neumann-Morgenstern utility theorem~\cite{von1947theory}. We introduce lotteries over invitations and define preferences of agents over lotteries. Given a probability distribution over invitations, one can compute the expected cardinal utility of lotteries. We then define a strategy-proof mechanism analogously to Definition~\ref{GT:def:mechanism}: for each $a_i$, it must hold that the expected utility of $Z(P_i, v_{-i})$ is no less than the expected utility of $Z(v_i, v_{-i})$ for all $v_i\in V_i$ and for all $v_{-i} \in V_{-i}$. The impossibility result given by Theorem~\ref{GT:thm:impossibility} still holds: If $(V, Z)$ is a strategy-proof mechanism, then $Z(P_1, P_2)$ must assign zero probability to both $\{a_1\}$ and $\{a_2\}$, yet these are the only two stable invitations.  All other impossibility results we mention in this work can be extended in this manner.

We can extend our results in a different direction by considering multiple time-alternatives for the event. In such settings, agents may have preferences over time-alternatives for the event, in addition to size of invitations.   
In the non-strategic case, our easiness result is still applicable: One can run the algorithm used in Theorem~(to be added  GT:thm:asip:algo) iteratively for each time-alternative, and choose the maximum stable invitation among all.
In the strategic case, our impossibility results immediately imply the same negative results. 
For \INC-instances of \ASIP, we obtain a similar impossibility result even if there is only two time-alternatives. The intuition is that over-reporting ($L_i > l_i$) can give the veto power to an agent, which prevents us from designing a strategy-proof mechanism even for \INC-instances of \ASIP. Note that all of our impossibility results can be naturally extended to the Group Activity Selection Problem by Darmann et al.~\cite{GASP12WINE} since \ASIPs is a sub-class of \GASPs with a single activity.


\subsection{Extensions to Stable Invitations Problem}


\subsection{Extensions to Group Activity Selection Problem}


\section{Discussion}

While we focused on designing algorithms or proving computational hardness results in previous chapters, the focus of this chapter is on mechanism design and impossibility results (much like computational hardness results). In mechanism design theory, the objective is to design a mechanism which is given an action profile of all agents and chooses an outcome (which can be probabilistic) such that the mechanism satisfies certain properties. One of the most important properties is incentive compatibility -- agents must be incentivized to act in a certain way (such as truth-telling) because no agent can be better off by acting in a different way. In many settings, utilites of agents may be transferable (such as money), which provides ways to compensate for agents who end up with less desirable outcomes. However, in social settings like the ßgroup scheduling and assignment problems, it is more reasonable to assume that utilites of agents are assumed to be non-transferable, and therefore it is not possible to compenstate agents for any outcome. 
