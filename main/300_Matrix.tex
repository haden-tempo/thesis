\label{matrix:chapter}

In Chapter~\ref{bdoodle:chapter}, we assumed that an event organizer is only allowed to inspect a group of columns (as a batch of date/time options) by polling all invitees at once.
In this chapter, we generalize the notion of ``inspection'' by the event organizer, and assume that the organizer can inspect each entry of a random matrix at unit cost (by querying a certain invitee about a certain date/time option at a unit cost).
Consequently, \Times and \Inconveniences are always equal in this setting, and the goal is to find an optimal inspection sequence which is a permutation of entries of a given random matrix.

In classic Doodle, an inspection is performed over the entire random matrix by querying all invitees (rows) about all date/time options (columns).
In the Batched Doodle Problem, we considered a class of B-Doodle mechanisms in which all invitees are queried about a subset of date/time options.
In this chapter, we study the Probabilistic Matrix Inspection Problem in which an individual invitee is queried over a single date/time option. In practice, an organizer may want to ask the most important (and/or busiest) invitee first about a couple of date/time options, to ensure that the person is available, before asking other invitees. For instance, when a doctoral graduate student is scheduling his/her university oral examination at Stanford University (which is usually the defense of dissertation in computer science), it is required that both the student's primary advisor and the exam chair must be physically present while other examiners may participate virtually. %TODO CITE as of some date.
In such cases, the student can ask the advisor and exam chair to rule out certain date/time options that do not work for them, and then ask other examiners about their availability. 

While group scheduling is a motivating example for the Probabilistic Matrix Inspection Problem, there are many other applications that can be modeled in this manner. For instance, suppose that some research lab is trying to hire a new researcher and they are considering a number of candidates.
After reading each candidate's curricular vitae and research statement,
each member of the lab can estimate the likelihood of him/her approving the candidate. It is still necessary to interview a candidate to be certain of his or her quality.
In this simplified setting, the problem of finding a candidate who is approved by every member of the lab is equivalent to the problem of finding a feasible date/time option. Clearly, assuming that the lab is happy to hire any of the candidates whom all of the lab members approve, it is rational to minimize the expected number of in-person interviews until someone is hired or every candidate is rejected. 
Another example can include a search quest for housing or childcare facility.
Each house or facility is a candidate, and there is a set of requirements for the candidate being considered. Making inquiries about and taking a tour of the property or facility involves much effort -- even though much information is available through brochures or booklets, people in this situation would wish to minimize the time and effort they need to spend until they declare a winner.

In this chapter, we study the Probabilistic Matrix Inspection Problem which models the abstract essence of all applications we described earlier.
In Section~\ref{matrix:sec:notation}, we introduce notation and formally define the problem. 
In Section~\ref{matrix:sec:results}, we present our technical results.
In Section~\ref{matrix:sec:discussion}, we discuss how our technical results can be used for the real-life scenarios we described in this introduction. 





\section{Notation and Definitions} \label{matrix:sec:notation}



  We use the same definition of feasibility as in previous chapter, which is copied below. 

 \begin{definition}[Feasibility]
 Let $A$ be a matrix whose entries are from $\{0,1\}$, and refer to the entry of $A$ at row $r$ and column $c$ as $a_{r,c}$.
 We say that a column $c$ of $A$ is {\em feasible} if it consists only of 1's (otherwise it is {\em infeasible}). 
 We say that $A$ is {\em feasible} if it contains at least one feasible column (otherwise it is {\em infeasible}).
 \end{definition}

 In the Probabilistic Matrix Inspection Problem, we do not know of the values of the entries of $A$, as they are Bernoulli random variables, but we know of probability distribution of each entry.
 An input to the problem is this probability distribution. 
 \begin{definition}[Input instance]
 An input instance of the Probabilistic Matrix Inspection problem is a pair of matrices $(A, P_A)$ of size $n$ by $m$
 where $A$ is a matrix of Bernoulli random variables 
 and $P_A$ is the probability matrix associated with it.
 We denote an entry of $A$ as $a_{r,c}$ and of $P_A$ as $p_{r,c}$.
 Each entry $a_{r,c}$ of $A$ is a Bernoulli random variable, and $p_{r,c}$ is the probability of success for $a_{r,c}$ (i.e., $p_{r,c} = \mathbf{P}[a_{r,c} = 1]$).
 We define $s_c = \prod_{r=1}^{n} p_{r,c}$ to denote the probability that column $c$ is feasible (probability of success for column $c$).
 \end{definition}
 Throughout this work we will assume that the set of random variables $\{a_{r,c}\}$ are mutually independent. This assumption is crucial to our technical results because it allows to compute (in polynomial time) the probability of a specific realization of $A$ conditioning on the event in which some entries of $A$ have already been realized. 
 Without this assumption, it is unclear how one can compute such probabilities without having an access to the joint probability distribution over all realizations of $A$ (whose size is exponential in the size of $A$).

 We define an ``inspection'' as an operation that can be performed on $A$. 
 One can inspect an arbitrary entry $a_{r,c}$ of $A$ at unit cost, so as to know of the realization of the random variable. In group scheduling, an inspection corresponds to querying an agent about her availability for a certain outcome. 
 The objective of the problem is to determine whether $A$ is feasible or not, with minimum (expected) number of inspections possible. The expectation is with respect to the probability distribution specified by $P_A$.

 Because we are interested in determining feasibility of $A$ with minimum number of inspections, there are certain ``unnecessary inspections'' that an optimal strategy must avoid.
 For instance, if a certain entry $a_{r,c}$ is found to be unsuccessful (i.e., $a_{r,c} = 0$ is realized), then there is no need to inspect any other entry from the same column because the column is already known to be infeasible. Similarly, if a certain column is found to be feasible (which implies $A$ is feasible) or if all columns are found to be infeasible (which implies $A$ is infeasible), then there is no need to inspect any other entries of the matrix. Lastly, if $p_{r,c} = 0$ or $p_{r,c} = 1$, then there is no need to inspect the entry $a_{r,c}$ because we already know its realization with probability $1$.
 Therefore, without loss of generality, we will assume that $p_{r,c} \in (0,1)$ (i.e., $p_{r,c}\neq 0,1$) in this work. 

 Let us define what constitutes a solution to the problem. 
 \begin{definition}[Inspection policy]
 	Given an input instance $(A,P_{A})$, a solution is any permutation of the entries of $A$, and we call it an ``inspection policy'' (or simply, a ``policy'').
 \end{definition}
 The interpretation of a permutation is as follows. The entries of $A$ will be inspected in order specified by the permutation. After each inspection, if $A$ is found to be feasible or infeasible, the inspection process ends. Otherwise, it continues inspecting the entries as specified, but it will not make any unnecessary inspections as mentioned earlier. 

 If $\pi$ is a permutation of the entries of $A$, we write $C(\pi)$ to denote the number of inspections performed by $\pi$. $C(\pi)$ is a random variable whose probability distribution is determined by $P_A$. We are interested in finding an optimal policy which minimizes the expected number of inspections, $\mathbf{E}[C(\pi)]$.
 Note that there are $(nm)!$ permutations of the entries of $A$, and therefore exhaustive search for an optimal permutation will not produce an efficient algorithm. 

 \subsection{Example} \label{matrix:sec:example}

 Consider a 2-by-2 matrix $A$ of Bernoulli random variables whose probability of success is given by $P_A$ as follows. In group scheduling, this corresponds to two agents and a set of two date/time options being considered.
 \begin{equation*}
 A = 
 	\begin{bmatrix}
 		a_{1,1}  & a_{1, 2} \\
 		a_{2,1}  & a_{2, 2}
 	\end{bmatrix},
 P_A = 
 	\begin{bmatrix}
 		0.6  & 0.7  \\
 		0.9  & 0.8 
 	\end{bmatrix}
 \end{equation*}
 Let us consider an inspection policy $\pi$ which inspects the entries of $A$ column-by-column while inspecting them from top to bottom within a column:
 \begin{equation*}
 	\pi = 
 	\begin{pmatrix} 
 	1 & 2 & 3 & 4  \\
 	a_{1,1} & a_{2,1} & a_{1,2} & a_{2,2} 
 	\end{pmatrix}.
 \end{equation*}	
 Suppose that the realization of $A$ happens to be the identity matrix of size $2$ (i.e., $a_{1,1} = a_{2,2} = 1$ and $a_{2,1} = a_{1,2} = 0$). If we use $\pi$, it will first inspect $a_{1,1}$ and learn its realization. Since $a_{1,1} = 1$, it will inspect $a_{2,1}$ next only to find that column $1$ is infeasible after all. It will then inspect $a_{1,2}$ and learn that column $2$ is also infeasible, which implies that $A$ is infeasible. At this point, the inspection process terminates without inspecting $a_{2,2}$.
 This specific realization of $A$ happens with probability $p_{1,1}(1 - p_{2,1})(1-p_{1,2})p_{2,2}$, and yields $C(\pi) = 3$ because 3 inspections would occur. 
 If the realization of $A$ happens to be the null matrix (i.e., all entries are $0$'s), then $\pi$ would only inspect $a_{1,1}$ and $a_{1,2}$ but skip $a_{2,1}$ and $a_{2,2}$.  In this manner one can consider all $2^{2\cdot 2} = 16$ possible realizations of $A$, and compute $\mathbf{E}[C(\pi)]$ in this example. 

 Another way to compute $\mathbf{E}[C(\pi)]$ is by de-coupling $C(\pi)$ into two random variables $N(\pi, c)$ with $c\in\{1,2\}$ where $N(\pi,c)$ denotes the number of inspections (on column $c$) performed by $\pi$ conditioning on the event that (at least one element of) column $c$ is inspected.
 We can efficiently compute these: $\mathbf{E}[N(\pi, c)] = 1 \cdot \mathbf{P}[a_{1,c} = 0] + 2 \cdot \mathbf{P}[a_{1,c} = 1]$ for $c\in \{1, 2\}$.
 To express $\mathbf{E}[C(\pi)]$ in $N(\pi,c)$'s, we need to take conditional probability into account, as column 2 is inspected only if column 1 is infeasible: 
 $\mathbf{E}[C(\pi)] = \mathbf{E}[N(\pi,1)] + \mathbf{E}[N(\pi,2)] (1-s_1) = 2.382$. Recall that $s_c$ is the probability of success for column $c$.


\section{Technical Results and Algorithm} \label{matrix:sec:results}


 We first consider two special cases (1-row or 1-column matrices) of the Probabilistic Matrix Inspection problem, which admit intuitive, greedy algorithms. We then discuss a couple of interesting properties of an optimal inspection policy, which leads to our main result and algorithm.

 \subsection{1-Row Matrix and 1-Column Matrix}
 Let us first consider the case where an input matrix $A$ has only one row (i.e., $n = 1$).
 In this case it is natural to inspect entries with largest probability first because we can stop as soon as we find an entry whose value is $1$ -- which makes its column and $A$ feasible. This intuition is exactly what an optimal policy should do in the single row case.

 \begin{lemma}[1-Row Matrix]\label{matrix:lemma:single_row_opt}
 When $n = 1$, an inspection policy $\pi$ is optimal if and only if it inspects the entries in non-increasing order of their associated probabilities.
 \end{lemma}
 \begin{proof}
 	Without loss of generality let us assume that a policy $\pi$ inspects the entries in increasing order of their column index; that is, $\pi(i) = a_{1,i}$. 
 	Recall that $C(\pi)$ is a random variable that denotes the number of inspections $\pi$ incurs. 
 	We can express the expectation of $C(\pi)$ in terms of $p_{1,c}$'s as follows:
 	\small
 	\begin{equation} \label{matrix:eqn:exp_cost_pi_1row}
 		\mathbf{E}\left[C(\pi)\right] = 
 		m \left(\prod_{k=1}^{m-1}(1 - p_{1,k})\right) + 
 		\sum_{j=1}^{m-1} j \left(p_j \prod_{k=1}^{j-1} (1-p_{1,k}) \right).
 	\end{equation}
 	\normalsize
	
 	Suppose that there exists some $c^*$ such that $p_{1,c^*} < p_{1,c^*+1}$ (if no such $c^*$ exists, then $\pi$ is an inspection policy that inspects the entries in non-increasing order of probabilities).
 	Let $\pi'$ be the same policy as $\pi$ except we swap the order of $a_{1,c^*}$ and $a_{1,c^*+1}$. 
 	That is, $\pi'$ is defined as follows.
 	\begin{equation*}
 		\pi'(j) = 
 		\begin{cases}
 			\pi'(j) = \pi(c^*+1)&  \mbox{if~} j = c^* \\
 			\pi'(j) = \pi(c^*)  &  \mbox{if~} j = c^*+1 \\
 			\pi'(j) = \pi(j)  &  \mbox{if~} j \neq c^* \land j \neq c^*+1
 		\end{cases}
 	\end{equation*}	
 	After expressing $\mathbf{E}[C(\pi)]$ and $\mathbf{E}[C(\pi')]$ as in Equation~\ref{matrix:eqn:exp_cost_pi_1row}, one can re-arrange the terms to obtain the following:
 	\begin{equation} \label{matrix:eqn:proof_pi_1row}
 	\mathbf{E}\left[C(\pi)\right] - \mathbf{E}\left[C(\pi')\right] = 
 	\left(\prod_{j=1}^{c^*-1} (1-p_{1,j})\right) (p_{1,c^*+1} - p_{1,c^*}).
 	\end{equation}
 	This quantity is positive if $p_{1,c^*+1} > p_{1,c^*}$ (recall that $p_{1,j} \in (0,1)$ for all $j$ as mentioned in Section~\ref{matrix:sec:notation}). 
	
 	This proves the lemma because any policy that inspects an entry with smaller probability before another entry with higher probability is suboptimal, and therefore an optimal policy must inspect entries in non-increasing order of their associated probabilities. 
 \end{proof}
 Although the proof of Lemma~\ref{matrix:lemma:single_row_opt} is simple, it confirms correctness of our intuition. Equation~\ref{matrix:eqn:proof_pi_1row} illustrates this intuition; conditioning on the event that the first $c^*-1$ inspections fail (whose probability is the product term in Equation~\ref{matrix:eqn:proof_pi_1row}), the difference $\mathbf{E}[C(\pi)] - \mathbf{E}[C(\pi')]$ depends on the difference in the probabilities of success between the next-entry-to-be-inspected by $\pi$ and $\pi'$.

 We can also consider the case where an input matrix $A$ has only one column.
 Intuitively, if we wish to minimize the expected number of inspections, we must inspect entries with smallest probability first because we can stop as soon as we determine that $A$ is infeasible.
 Lemma~\ref{matrix:lemma:single_column_opt} formally states this intuition about optimal policy, and we omit a proof of it as it can be easily done by following the proof of Lemma~\ref{matrix:lemma:single_row_opt}. %TODO: do not omit?
 \begin{lemma}[1-Column Matrix] \label{matrix:lemma:single_column_opt}
 When $m = 1$, an inspection policy $\pi$ is optimal if and only if it inspects the entries in non-decreasing order of their associated probabilities.
 \end{lemma}


 \subsection{Inspection of Entire Column}
 Another interesting property of an optimal inspection policy is that once it inspects the first entry of a column, then it must commit to it and continue inspecting the remaining entries of the column until feasibility of the column is determined. Otherwise, if the policy switches to another column too soon, then it is not optimal. 
 \begin{theorem}[Optimality of inspecting entire column] \label{matrix:theorem:col_by_col_opt}
 Consider any inspection policy $\pi$.
 Without loss of generality, let us assume that for each column $c$, $\pi$ inspects $a_{n,c}$ the last among $n$ entries of the column. 
 Let $b_c$ be the index of $\pi$ such that $\pi(b_c) = a_{n,c}$. 
 Without loss of generality, assume $b_1 < b_2 < \cdots < b_m$ (we can do this by re-labeling the columns of $A$). 
 If there is some column $c^*$ such that $b_{c^*} > n\cdot c^*$, then $\pi$ is not optimal.
 \end{theorem}
 \begin{proof}
 First, note that $b_c \geq n\cdot c$ for all $c$ because we assumed $b_1 < b_2 < \cdots < b_m$, and therefore the entries of previous columns must appear before the last entry of each column.

 Let $\pi$ be an inspection policy being considered in the theorem for which there exists some $c$ with $b_{c} > n \cdot c$.
 Let us construct a different inspection policy $\pi'$.
 First, $\pi'$ inspects all entries of column $1$ in the same order $\pi$ does. 
 Then, $\pi'$ inspects all entries of column $2$ in the same order $\pi$ does, and so on. 
 In particular, $\pi'$ inspects all entries of a column before inspecting another column, while preserving the original ordering of the entries within each column that is given by $\pi$. 
 We will show that $\mathbf{E}[C(\pi')] < \mathbf{E}[C(\pi)]$.

 Let us define a set of new random variables which can be used to express $C(\cdot)$, as we did in Section~\ref{matrix:sec:example} when analyzing an example.
 Recall that $s_c = \prod_{r=1}^{n} a_{r,c}$ is the probability of success for column $c$.
 Let $N(\pi, c)$ ($N(\pi', c)$, respectively) be a random variable that denotes the number of entries of column $c$ that is inspected by $\pi$ (by $\pi'$, respectively), conditioning on the event that column $c$ is inspected (i.e., when the previous $c-1$ columns are infeasible).
 We can then express $\mathbf{E}[C(\pi)]$ and $\mathbf{E}[C(\pi')]$ as follows:
 \begin{equation} \label{matrix:eqn:exp_c_pi_decoupled}
 	\mathbf{E}[C(\pi)] = \sum_{c=1}^{m} \mathbf{E}[N(\pi, c)]\left( \prod_{k=1}^{c-1} (1 - s_c) \right)
 \end{equation}
 and
 \begin{equation} \label{matrix:eqn:exp_c_pi2_decoupled}
 	\mathbf{E}[C(\pi')] = \sum_{c=1}^{m} \mathbf{E}[N(\pi', c)]\left( \prod_{k=1}^{c-1} (1 - s_c) \right).
 \end{equation}


 To prove the theorem we will first show that for any realization of $A$, $N(\pi, c) \geq N(\pi', c)$ holds for all $c$; this immediately implies $\mathbf{E}[C(\pi)] \geq \mathbf{E}[C(\pi')]$.
 We will then show that there exists at least one realization of $A$ such that for some column $c'$ the strict inequality $N(\pi, c') > N(\pi', c')$ holds. These two statements together imply that $\mathbf{E}[C(\pi)] > \mathbf{E}[C(\pi')]$.


 Consider any realization of $A$ with the condition that the first $m-1$ columns are infeasible (recall that $m$ is the number of columns of $A$).
 Then $N(\pi, c) = N(\pi', c)$ for all $c$ regardless of feasibility of column $m$.
 To see why, both $\pi$ and $\pi'$ would inspect the same set of entries in each of the first $m-1$ columns in the same order until the column is determined to be infeasible, and therefore $N(\pi, c) = N(\pi', c)$ if $c<m$.
 If column $m$ is feasible, then both $\pi$ and $\pi'$ would inspect all $n$ entries of it, and thus we have $N(\pi, m) = N(\pi', m) = n$. Otherwise, if column $m$ is also infeasible (in which case $A$ is infeasible), then $\pi$ and $\pi'$ would inspect the same set of entries of column $m$ in the same order until the first infeasible entry of the column is found. Therefore if the first $m-1$ columns are infeasible we have $N(\pi, c) \geq N(\pi', c)$ for all $c$.

 Now consider any realization of $A$ with the condition that at least one of the first $m-1$ columns is feasible. Let $c'$ be the smallest index of feasible columns of $A$.
 Because the columns from $1$ to $c'-1$ are infeasible, $N(\pi, c) = N(\pi', c)$ for all $c < c'$ for the same reason we stated earlier for the other case. 
 Since $c'$ is feasible, $N(\pi, c') = N(\pi', c') = n$ as both policies would inspect all $n$ entries of $c'$. By our construction of $\pi'$ it is clear that $N(\pi', c) = 0$ for all $c > c'$; therefore we have $N(\pi, c) \geq N(\pi', c)$ for all $c>c'$. In summary $N(\pi, c) \geq N(\pi', c)$ holds for all $c$ in this case as well. 

 So far we proved the first claim we stated earlier: for all realizations of $A$, we have $N(\pi, c) \geq N(\pi', c)$ for all $c$.
 Let us now prove the second claim. Let $c^*$ be the smallest index $c$ of columns such that $b_c > nc$ (note that $c^* < m$ because $b_m = nm$ by definition). 
 Consider any realization of $A$ with the condition that the first $c^*-1$ columns are infeasible and column $c^*$ is feasible (feasibility of other columns do not matter). 
 Using the same arguments we used earlier, we can show that $N(\pi, c) = N(\pi', c)$ for all $c < c^*$, that $N(\pi, c^*) = N(\pi', c^*) = n$, and that $N(\pi', c) = 0$ for all $c> c^*$. 
 However, because $b_{c^*} > n \cdot c^*$, there is at least one entry $a_{r',c'}$ with $c' > c^*$ which  appears before $b_{n, c^*}$ in $\pi$ (otherwise, if no such entry exists, then $b_{c^*}$ would be equal to $n \cdot c^*$). This implies that there exists some $c'$ with $c' > c^*$ such that $N(\pi, c') > 0$. 
 This proves the second claim that for some realization of $A$, there is some column $c'$ for which $N(\pi, c') >  N(\pi', c')$, and together with the first claim we proved earlier, this implies that $\mathbf{E}[C(\pi)] > \mathbf{E}[C(\pi')]$. 

 This proves the theorem: Any policy that does not inspect all entries of a column consecutively is suboptimal.
 \end{proof}
 By Theorem~\ref{matrix:theorem:col_by_col_opt}, when seeking an optimal policy, it is sufficient to consider the set of policies that inspect an entire column before committing to another column. Lemma~\ref{matrix:lemma:single_column_opt} hints that one should inspect the entries of each column in increasing order of probabilities, and this is what we prove next.


 \subsection{Optimal Ordering within Column} 

 Lemma~\ref{matrix:lemma:single_column_opt} states that
 an optimal policy must inspect the entries in increasing order of their probability of success, if $A$ is a 1-column matrix.
 This argument can be generalized to the case where there is more than one column: If an optimal policy is to inspect an entry of some column $c$, it must inspect the entry with smallest probability of success first. 
 \begin{theorem}[Optimal ordering within column] \label{matrix:theorem:within_column_opt}
 	Consider any inspection policy $\pi$.
 	If there exist two entries $a_{r_1,c}$ and $a_{r_2,c}$ from the same column such that $a_{r_1,c}$ appears before $a_{r_2,c}$ in $\pi$ and $p_{r_1,c} > p_{r_2,c}$, then $\pi$ is not optimal. 
 	In other words, when restricted to each column, an optimal policy must inspect the entries of the column in non-decreasing order of probabilities.
 \end{theorem}
 \begin{proof}
 	Let $\pi$ be an inspection policy being considered in the theorem.
 	Because of Theorem~\ref{matrix:theorem:col_by_col_opt} we can assume, without loss of generality, that $\pi$ inspects all entries of column 1, followed by column 2, and so on.
 	Further let us assume that $\pi$ inspects the entries of each column in increasing order of their row index (we can do so by re-labeling the indices of entries). Precisely, $\pi(r + n (c-1)) = a_{r,c}$ defines $\pi$. Let $p_{r,c^*}$ and $p_{r+1,c^*}$ be the entries with $p_{r,c^*} > p_{r+1, c^*}$.
 	Let us consider a different inspection policy $\pi'$ that is the same as $\pi$ except that $\pi'$ inspects $p_{r+1,c^*}$ before $p_{r,c^*}$, by swapping the ordering of them.
 	\begin{equation*}
 		\pi'(j) = 
 		\begin{cases}
 			\pi'(j) = a_{r+1,c^*}  &  \mbox{if~} \pi(j) = a_{r,c^*} \\
 			\pi'(j) = a_{r,c^*}    &  \mbox{if~} \pi(j) = a_{r+1,c^*} \\
 			\pi'(j) = \pi(j)     &  \mbox{otherwise} 
 		\end{cases}
 	\end{equation*}	
	
 	We claim that $\mathbf{E}[C(\pi')] < \mathbf{E}[C(\pi)]$, which implies that $\pi$ is not optimal.
	
 	Let us define new random variables $N(\pi, c)$ and $N(\pi', c)$ as we did in our proof of Theorem~\ref{matrix:theorem:col_by_col_opt} (i.e., the number of inspections performed by the respective policy on column $c$, conditioning on the event that the column is inspected).
 	Then we can express $\mathbf{E}[C(\pi)]$ and $\mathbf{E}[C(\pi')]$ in terms of the new random variables and $s_c$'s as we did in Equations~\ref{matrix:eqn:exp_c_pi_decoupled} and \ref{matrix:eqn:exp_c_pi2_decoupled}.
	
 	Observe that $N(\pi, c) = N(\pi', c)$ for any realization of $A$ if $c \neq c^*$.
 	To see this, first note that column $c$ would not be inspected by $\pi$ or by $\pi'$ if any of the previous columns (that is, columns $1$ through $c-1$) is found to be feasible, in which case $N(\pi, c) = N(\pi', c) = 0$. Otherwise, if column $c$ is inspected, both policies would inspect the entries of $c$ in the very same order, so $N(\pi, c) = N(\pi', c)$ must hold. Therefore we conclude that $\mathbf{E}[N(\pi, c)] = \mathbf{E}[N(\pi', c)]$ when $c \neq c^*$. 
	
 	We will now show that $\mathbf{E}[N(\pi, c^*)] > \mathbf{E}[N(\pi', c^*)]$ holds. 
 	This immediately implies $\mathbf{E}[C(\pi)] > \mathbf{E}[C(\pi')]$ due to Equations~\ref{matrix:eqn:exp_c_pi_decoupled} and \ref{matrix:eqn:exp_c_pi2_decoupled}.
 	Let us express $\mathbf{E}[N(\pi, c^*)]$ in terms of $p_{r,c^*}$'s.
 	\begin{equation*}
 		\mathbf{E}[N(\pi, c^*)] = n\left( \prod_{k=1}^{n-1} p_{k,c^*}\right) + 
 		\sum_{j=1}^{n-1} j (1-p_{j,c^*}) \left( \prod_{k=1}^{j-1} p_{k,c^*} \right)
 	\end{equation*}
 	Note that the event $N(\pi, c^*) = j$ occurs if the first $j-1$ entries are feasible while the $j$-th entry is not feasible when $j < n$, and $N(\pi, c^*) = n$ occurs if the first $n-1$ entries are feasible (but the $n$-th entry's feasibility does not matter).
	
 	We can express $\mathbf{E}[N(\pi', c^*)]$ in a similar manner, and simplify  $\mathbf{E}[N(\pi, c^*)] - \mathbf{E}[N(\pi', c^*)]$ as follows:
 	\small
 	\begin{equation*}
 		\mathbf{E}[N(\pi, c^*)] - \mathbf{E}[N(\pi', c^*)] = 
 		r \left( \prod_{k=1}^{r-1} p_{k,c^*} \right) (p_{r,c^*} - p_{r+1,c^*}).
 	\end{equation*}
 	\normalsize
 	The quantity above is positive if $p_{r,c^*} > p_{r+1,c^*}$, which is the assumption we began with.
 	This proves the theorem.
 \end{proof}


 Theorems~\ref{matrix:theorem:col_by_col_opt} and \ref{matrix:theorem:within_column_opt} together tell us that 
 in order to find an optimal policy we only need to decide the ordering of the columns. 
 There are still $m!$ orderings of columns, and an exhaustive search algorithm would not be efficient.
 As we were able to generalize Lemma~\ref{matrix:lemma:single_column_opt} to Theorem~\ref{matrix:theorem:within_column_opt} by generalizing the optimal solution for 1-column case, it would be natural to consider generalizing Lemma~\ref{matrix:lemma:single_row_opt} in a similar manner.

 This idea leads to the following greedy algorithm: First we sort columns by their probability of success ($s_c = \prod_{r=1}^{n} p_{r,c}$) in decreasing order, and inspect the entries of each column in increasing order of their associated probabilities. However, as the following example shows, this algorithm is suboptimal. 
 \begin{equation*}
 A = 
 	\begin{bmatrix}
 		a_{1,1}  & a_{1, 2} \\
 		a_{2,1}  & a_{2, 2}
 	\end{bmatrix},
 	P_A =
 	\begin{bmatrix}
 		0.4459  & 0.2262 \\
 		0.4459  & 0.8114
 	\end{bmatrix}
 \end{equation*}
 Here we have $s_1 ~= 0.199$ and $s_2 ~= 0.184$, and the greedy algorithm would produce $\pi = \begin{pmatrix} a_{1,1} & a_{2,1} & a_{1,2} & a_{2,2} \end{pmatrix}$. Its expected cost, $\mathbf{E}[C(\pi)]$, is $2.428$, but if we inspect the second column first, then the expected cost is $2.407$ which is optimal in this example.
 One can consider another greedy algorithm which inspects the columns in increasing order of their expected number of inspections (within column), but this algorithm turns out to be suboptimal as well. 


 \subsection{Main Result and Algorithm}
 Let us present the main result that leads to an efficient algorithm for finding an optimal inspection policy. 

 \begin{theorem} \label{matrix:thm:main_result}
 	Let $s_c$ be the probability of success for column $c$ as before. 
 	Let $\mu_c$ be the expected number of inspections that column $c$ incurs if its entries are inspected in increasing order of their probability of success, conditioning on the event that column $c$ is inspected and infeasible.
 	An optimal policy must be a column-by-column policy (due to Theorem~\ref{matrix:theorem:col_by_col_opt}), must inspect the entries of each column in non-decreasing order of probabilities (due to Theorem~\ref{matrix:theorem:within_column_opt}), and must inspect the columns in non-decreasing order of $\mu_c(1 - s_c)/s_c$.
 \end{theorem}
 \begin{proof}
 	Consider a column-by-column inspection policy $\pi$ which inspects the column 1 through $m$ in increasing order of their index (we can assume this without loss of generality by re-labeling columns).
	
 	As before, let $N(\pi,c)$ be a random variable that denotes the number of inspections performed by $\pi$ on column $c$, conditioning on the event that column $c$ is inspected.
 	Then we can express $\mathbf{E}[N(\pi, c)]$ in terms of $s_c$ and $\mu_c$ as follows.
 	\begin{equation}\label{matrix:eqn:exp_cost_column_by_condition}
 		\mathbf{E}[N(\pi, c)] = s_c \cdot n  + (1-s_c) \cdot \mu_c
 	\end{equation}
 	This equation holds because if the column is feasible (with probability $s_c$), it would require $n$ inspections, but if it is not (with probability $1-s_c$), it would require $\mu_c$ inspections in expectation. The equation above simply considers these two events, and calculates the expected value of $N(\pi, c)$. 
	
 	Suppose that there is some column $c^*$ such that $\mu_{c^*}(1 - s_{c^*}) / s_{c^*} > \mu_{c^*+1} (1 - s_{c^*+1}) / s_{c^*+1}$. Because $\pi$ inspects column $c^*$ before column $c^*+1$, it would not be inspecting the columns in increasing order of $\mu_c(1-s_c)/s_c$.
 	Consider a different inspection policy $\pi'$ which inspects the columns in the same order as $\pi$ except that $\pi'$ inspects column $c^*+1$ before $c^*$ by swapping the inspection ordering of the two.
 	We can relate $N(\pi, \cdot)$ to $N(\pi', \cdot)$ as follows.
 	\begin{equation*}
 		N(\pi', c) = 
 		\begin{cases}
 			N(\pi, c^*+1) & \mbox{if~} c = c^* \\
 			N(\pi, c^*)   & \mbox{if~} c = c^* + 1 \\
 			N(\pi, c)     & \mbox{otherwise}
 		\end{cases}
 	\end{equation*}
	
 	As we did in proofs of Theorems~\ref{matrix:theorem:col_by_col_opt} and \ref{matrix:theorem:within_column_opt}, we can use Equations~\ref{matrix:eqn:exp_c_pi_decoupled} and \ref{matrix:eqn:exp_c_pi2_decoupled}, and simplify $\mathbf{E}[C(\pi)] - \mathbf{E}[C(\pi')]$ as follows.
	\small
 	\begin{equation} \label{matrix:eqn:main_result_diff}
 		\begin{aligned}
 		\mathbf{E}[C(\pi)] - \mathbf{E}[C(\pi')] ={} & \left(\frac{\mathbf{E}[N(\pi, c^*)]}{s_{c^*}}  - \frac{\mathbf{E}[N(\pi, c^*+1)]}{s_{c^*+1}} \right) \\
 		& \cdot \left( \prod_{j=1}^{c^*-1} (1 - s_j)\right) s_{c^*} s_{c^*+1}
 		 \end{aligned}
 	\end{equation}	
	\normalsize
 	The quantity in Equation~\ref{matrix:eqn:main_result_diff} is positive if the difference of the weighted expected values (in the first parentheses) are positive. Using Equation~\ref{matrix:eqn:exp_cost_column_by_condition} we obtain the following inequality.
 	\begin{equation*}
 		\begin{aligned}
 			{} &
 		\frac{\mathbf{E}[N(\pi, c^*)]}{s_{c^*}}  > \frac{\mathbf{E}[N(\pi, c^*+1)]}{s_{c^*+1}} \\
 		\Leftrightarrow &
 		\mu_{c^*} (1 - s_{c^*}) / s_{c^*} > \mu_{c^*+1}(1 - s_{c^*+1}) / s_{c^*+1} 
 		\end{aligned}
 	\end{equation*}
 	By definition of $c^*$, the second inequality above holds, which implies $\mathbf{E}[C(\pi)] > \mathbf{E}[C(\pi')]$.
 	Therefore, an optimal inspection policy must inspect the columns in non-decreasing order of $\mu_c(1 - s_c)/s_c$. 
 \end{proof}
 Because there is a unique ordering of columns if we sort them by $\mu_c(1-s_c)/s_c$ (up to ties), Theorem~\ref{matrix:thm:main_result} leads to the following algorithm: We inspect the columns in increasing order of $\mu_c(1-s_c)/s_c$, and in each column, we inspect the entries of it in increasing order of probabilities. 
 This algorithm can easily be implemented to run in polynomial time. 


\section{Discussion and Future Work} \label{matrix:sec:discussion}
In this work we defined the Probabilistic Matrix Inspection problem motivated by group scheduling and Doodle. 
We first considered two special cases, and discovered interesting properties of an optimal inspection policy which agree with our intuition. We then generalized our findings to design an efficient algorithm to solve the general case, and along the way we showed that two natural greedy algorithms fail to find an optimal solution. While we believe that our technical results make a great starting point for studying and optimizing a group scheduling process, there remain several open problems and future work to be done.

As we discussed in previous chapter, our model and algorithm rely on the assumption that probability estimates on availability of agents are available. We suggested several ideas motivated by previous work in the literature, but it will be important to deploy such ideas into a system, and integrate it with our algorithm. From a theoretical perspective, there remain several open problems. While we assumed that an inspection can be performed on a single entry at unit cost, one can generalize the cost model by allowing an inspection of any subset of entries whose cost depends on, for example, the number of entries being inspected. In the context of group scheduling, an inspection on many entries means querying multiple agents at the same time for one or many outcomes (but an inspection is not limited to the entries from the same column or row). This generalization is particularly useful when scheduling takes place in a hierarchical setting such as corporates. For instance the event organizer may feel that the cost of querying a supervisor is significantly different from that of querying a colleague.

Lastly, although we focused on relating our model to group scheduling, the Probabilistic Matrix Inspection problem has other applications. Finding the right childcare facility, for example, involves extensive inquiries as parents wish to gather more information about how they would handle certain situations, what benefits and environments they provide, and so on. Through advertisements or brochures parents may even be able to gauge the likelihood of a certain facility satisfying their needs. Yet they still need to inquire facilities for precise information, which can be modeled by our probabilistic matrix model where columns correspond to facilities and rows correspond to the needs of parents. 

